{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import xlsxwriter\n",
    "import glob\n",
    "import xlrd\n",
    "from datetime import datetime\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Python 3.9.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Read and combine all CSVs\u001b[39;00m\n\u001b[1;32m      8\u001b[0m df_list \u001b[38;5;241m=\u001b[39m [pd\u001b[38;5;241m.\u001b[39mread_csv(file) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m csv_files]\n\u001b[0;32m---> 10\u001b[0m df_all \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mportid\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mportdate\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopening\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhigh\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlow\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclosing\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvaluess\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvolume\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     11\u001b[0m df_all[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopening\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_all[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopening\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m df_all[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhigh\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_all[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhigh\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/reshape/concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/reshape/concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[0;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/reshape/concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# Path to the folder containing CSV files\n",
    "folder_path = f\"/Users/arielgraham/Documents/Infovesta Tasks/trequity-2/2020-2025/*.csv\"  # Adjust this path\n",
    "\n",
    "# Get list of all CSV file paths\n",
    "csv_files = glob.glob(folder_path)\n",
    "\n",
    "# Read and combine all CSVs\n",
    "df_list = [pd.read_csv(file) for file in csv_files]\n",
    "\n",
    "df_all = pd.concat(df_list, ignore_index=True)[['portid','portdate','opening','high','low','closing','valuess', 'volume']]\n",
    "df_all['opening'] = df_all['opening'].astype('float32')\n",
    "df_all['high'] = df_all['high'].astype('float32')\n",
    "df_all['low'] = df_all['low'].astype('float32')\n",
    "df_all['closing'] = df_all['closing'].astype('float32')\n",
    "df_all['valuess'] = pd.to_numeric(df_all['valuess'], downcast='integer')  # Convert to smallest int type\n",
    "df_all['volume'] = pd.to_numeric(df_all['volume'], downcast='integer')  # Convert to smallest int type\n",
    "\n",
    "df_all.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_all: 0.42 Gigabytes\n"
     ]
    }
   ],
   "source": [
    "df_all = df_all.sort_values(['portid','portdate'],ascending=[True,True])\n",
    "print(f\"df_all: {np.round(df_all.memory_usage(deep=True).sum()/1e9,2)} Gigabytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_all\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mportdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myear\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_all[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mportdate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39myear)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:1101\u001b[0m, in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1099\u001b[0m         result \u001b[38;5;241m=\u001b[39m convert_listlike(argc, \u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1101\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43marg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, np\u001b[38;5;241m.\u001b[39mbool_):\n\u001b[1;32m   1103\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(result)  \u001b[38;5;66;03m# TODO: avoid this kludge.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:429\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    426\u001b[0m arg \u001b[38;5;241m=\u001b[39m ensure_object(arg)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 429\u001b[0m     \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43m_guess_datetime_format_for_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdayfirst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:131\u001b[0m, in \u001b[0;36m_guess_datetime_format_for_array\u001b[0;34m(arr, dayfirst)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (first_non_null \u001b[38;5;241m:=\u001b[39m tslib\u001b[38;5;241m.\u001b[39mfirst_non_null(arr)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(first_non_nan_element \u001b[38;5;241m:=\u001b[39m arr[first_non_null]) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:  \u001b[38;5;66;03m# noqa: E721\u001b[39;00m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;66;03m# GH#32264 np.str_ object\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m         guessed_format \u001b[38;5;241m=\u001b[39m \u001b[43mguess_datetime_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfirst_non_nan_element\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdayfirst\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m guessed_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m guessed_format\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_all['portdate'].apply(lambda x: pd.to_datetime(x).year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"/Users/irfanhilman/Downloads/Database/Daily Stock Price Investdata\"\n",
    "\n",
    "# filenames = glob.glob(path + \"/*.xls\")\n",
    "# print('File names:', filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_investdata(data,file_name):\n",
    "    outputxlsx = []\n",
    "    for file in file_name:\n",
    "        date_ = pd.read_excel(file,nrows=0)\n",
    "        date_ = date_.columns[0][19:]\n",
    "        date_ = pd.to_datetime(date_, format='%d-%b-%Y').strftime('%Y-%m-%d')\n",
    "        df = pd.read_excel(file,skiprows=[0])\n",
    "        df['portdate'] = date_\n",
    "        df = df.rename(columns={'Kode':'portid','Open':'opening','High':'high','Low':'low','Close':'closing','Value':'valuess','Volume':'volume'})\n",
    "        df = df[['portid','portdate','opening','high','low','closing','valuess','volume']]\n",
    "        outputxlsx.append(df)\n",
    "    df_augment = pd.concat(outputxlsx)\n",
    "    df_augment['portid + portdate'] = df_augment['portid'].astype('str') + ' ' + df_augment['portdate'].astype('str')\n",
    "    df_augment = df_augment.drop_duplicates(subset='portid + portdate')\n",
    "    df_augment = df_augment.drop(columns='portid + portdate')\n",
    "    df_augment = df_augment.sort_values(by='portdate')\n",
    "    data = data.append(df_augment)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Augment data dari investdata ke database\n",
    "# df_all = augment_investdata(data=df_all,file_name=filenames)\n",
    "# df_all = df_all.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat kolom First date sebagai perkiraan tgl IPO\n",
    "df_all_i_list = []\n",
    "for i in list(df_all['portid'].unique()):\n",
    "    df_all_i = df_all[df_all['portid']==i]\n",
    "    df_all_i = df_all_i.sort_values(by='portdate')\n",
    "    df_all_i['First Date'] = df_all_i.iloc[[0]]['portdate'].values[0]\n",
    "    df_all_i['Distance to First Date'] = pd.to_datetime(df_all_i['portdate']) - pd.to_datetime(df_all_i['First Date'])\n",
    "    df_all_i['Distance to First Date'] = df_all_i['Distance to First Date'].astype('str')\n",
    "    df_all_i_list.append(df_all_i)\n",
    "df_all = pd.concat(df_all_i_list)\n",
    "df_all = df_all.sort_values(by='portdate')\n",
    "df_all = df_all.reset_index(drop=True)\n",
    "df_all.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"df_all: {np.round(df_all.memory_usage(deep=True).sum()/1e9,2)} Gigabytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all[~df_all['portid'].str.contains(r'-\\w+', regex=True)]\n",
    "print(f\"df_all: {np.round(df_all.memory_usage(deep=True).sum()/1e9,2)} Gigabytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jika sudah run pembuatan database Laporan Keuangan, mulai dari sini!! Jika belum, buka Scraping_Stockbit.ipynb\n",
    "income_statement = pd.read_csv('/Users/arielgraham/Documents/Infovesta Tasks/Laporan Keuangan/Income Statement.csv',index_col=0)\n",
    "income_statement['Kode + Quarter'] = income_statement['Kode'] + ' ' + income_statement['Quarter'].astype(str)\n",
    "\n",
    "balance_sheet = pd.read_csv('/Users/arielgraham/Documents/Infovesta Tasks/Laporan Keuangan/Balance Sheet.csv',index_col=0)\n",
    "balance_sheet['Kode + Quarter'] = balance_sheet['Kode'] + ' ' + balance_sheet['Quarter'].astype(str)\n",
    "\n",
    "cash_flow = pd.read_csv('/Users/arielgraham/Documents/Infovesta Tasks/Laporan Keuangan/Cash Flow.csv',index_col=0)\n",
    "cash_flow['Kode + Quarter'] = cash_flow['Kode'] + ' ' + cash_flow['Quarter'].astype(str)\n",
    "cash_flow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Aggregated values\n",
    "\n",
    "income_statement_agg_col = list(income_statement.columns[~income_statement.columns.str.contains(r' #', regex=True)])\n",
    "balance_sheet_agg_col = list(balance_sheet.columns[~balance_sheet.columns.str.contains(r' #', regex=True)])\n",
    "cash_flow_agg_col = list(cash_flow.columns[~cash_flow.columns.str.contains(r' #', regex=True)])\n",
    "\n",
    "print(\n",
    "    f\"Income Statements agg: {income_statement_agg_col},\\n\"\n",
    "    f\"Balance Sheet agg: {balance_sheet_agg_col},\\n\"\n",
    "    f\"Cash Flow agg: {cash_flow_agg_col}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate here if satisfied\n",
    "\n",
    "income_statement = income_statement[income_statement_agg_col].dropna(axis=1, how='all')\n",
    "balance_sheet = balance_sheet[balance_sheet_agg_col].dropna(axis=1, how='all')\n",
    "cash_flow = cash_flow[cash_flow_agg_col].dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tambah Quarter di lapkeu jika data di TrEquity melebihi tanggal di lapkeu. Tujuannya supaya begitu di-join, data tidak terpotong di quarter terakhir lapkeu.\n",
    "#Contoh: Quarter terakhir lapkeu adalah Q32024 sedangkan tanggal di TrEquity sampai 10-Okt-2024, berarti tambah Quarter Q4 2024\n",
    "add_quarter = ['Q1 2025','Q4 2024']     # --> Harus urut jika lebih dari satu!!!! Contoh : ['Q1 2025','Q4 2024']\n",
    "\n",
    "s_list = []\n",
    "\n",
    "for s, data_s in income_statement.groupby('Kode', group_keys=False):\n",
    "    data_q = pd.DataFrame({'Kode': [s] * len(add_quarter), 'Quarter': add_quarter})\n",
    "    data_q['Kode + Quarter'] = data_q['Kode'] + ' ' + data_q['Quarter']\n",
    "    data_q = data_q.reindex(columns=income_statement.columns, fill_value=pd.NA).astype(object)  \n",
    "    data_q = data_q.dropna(axis=1, how='all') # Drop if ALL COL ISNA\n",
    "    s_list.append(pd.concat([data_q, data_s], ignore_index=True))  \n",
    "income_statement = pd.concat(s_list, ignore_index=True)\n",
    "\n",
    "s_list = []\n",
    "for s, data_s in balance_sheet.groupby('Kode', group_keys=False):\n",
    "    data_q = pd.DataFrame({'Kode': [s] * len(add_quarter), 'Quarter': add_quarter})\n",
    "    data_q['Kode + Quarter'] = data_q['Kode'] + ' ' + data_q['Quarter']\n",
    "    data_q = data_q.reindex(columns=balance_sheet.columns, fill_value=pd.NA).astype(object)  \n",
    "    data_q = data_q.dropna(axis=1, how='all') # Drop if ALL COL ISNA\n",
    "    s_list.append(pd.concat([data_q, data_s], ignore_index=True))\n",
    "balance_sheet = pd.concat(s_list)\n",
    "\n",
    "s_list = []\n",
    "for s, data_s in cash_flow.groupby('Kode', group_keys=False):\n",
    "    data_q = pd.DataFrame({'Kode': [s] * len(add_quarter), 'Quarter': add_quarter})\n",
    "    data_q['Kode + Quarter'] = data_q['Kode'] + ' ' + data_q['Quarter']\n",
    "    data_q = data_q.reindex(columns=balance_sheet.columns, fill_value=pd.NA).astype(object)  \n",
    "    data_q = data_q.dropna(axis=1, how='all') # Drop if ALL COL ISNA\n",
    "    s_list.append(pd.concat([data_q, data_s], ignore_index=True))\n",
    "cash_flow = pd.concat(s_list)\n",
    "\n",
    "lapkeu_sb = income_statement.copy()\n",
    "lapkeu_sb = lapkeu_sb.merge(balance_sheet.drop(['Kode','Quarter'],axis=1), how='outer', on='Kode + Quarter')\n",
    "lapkeu_sb = lapkeu_sb.merge(cash_flow.drop(['Kode','Quarter'],axis=1), how='outer', on='Kode + Quarter')\n",
    "lapkeu_sb['Kode'] = [c[:4] for c in lapkeu_sb['Kode + Quarter']]\n",
    "lapkeu_sb['Quarter'] = [q[5:7] for q in lapkeu_sb['Kode + Quarter']]\n",
    "lapkeu_sb['Year'] = [y[8:] for y in lapkeu_sb['Kode + Quarter']]\n",
    "lapkeu_sb['Quarter Year'] = lapkeu_sb['Quarter'].astype('str') + lapkeu_sb['Year'].astype('str')\n",
    "lapkeu_sb.insert(2, 'Year', lapkeu_sb.pop('Year'))\n",
    "lapkeu_sb.insert(3, 'Quarter Year', lapkeu_sb.pop('Quarter Year'))\n",
    "lapkeu_sb = lapkeu_sb.drop(columns=['Kode + Quarter'])\n",
    "lapkeu_sb.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"lapkeu_sb: {np.round(lapkeu_sb.memory_usage(deep=True).sum()/1e9,2)} Gigabytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Membuat dictionary berisi Quarter Number (ingat harus urut!! cek dulu di excel)\n",
    "quarter_num = {}\n",
    "keys = list(lapkeu_sb['Quarter Year'].unique())\n",
    "values = range(len(list(lapkeu_sb['Quarter Year'].unique())))\n",
    "index_ = [i for i in range(len(list(lapkeu_sb['Quarter Year'].unique())))]\n",
    "for eu,u in enumerate(keys):\n",
    "    quarter_num[u] = [values[eu]+1]\n",
    "#pd.DataFrame(quarter_num).T.to_excel('dict.xlsx')\n",
    "\n",
    "#Buat Quarter Number\n",
    "lapkeu_sb['Quarter Number'] = [quarter_num[x][0] for x in lapkeu_sb['Quarter Year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#buat function untuk annualizing parameter Income Statement dan Cash Flow\n",
    "def annualizing(report,annualizing_list):\n",
    "    for i in range(len(report)):\n",
    "        if i < (len(report)-4):\n",
    "            if (report.loc[i,'Kode'] == report.loc[i+3,'Kode']) and ((report.loc[i+1,'Quarter Number']-report.loc[i,'Quarter Number']) == 1) and  ((report.loc[i+2,'Quarter Number']-report.loc[i+1,'Quarter Number']) == 1) and ((report.loc[i+3,'Quarter Number']-report.loc[i+2,'Quarter Number']) == 1):\n",
    "                if (report.loc[i+1,'Quarter Number'] == ((report.loc[i,'Quarter Number']) + 1)) and (report.loc[i+2,'Quarter Number'] == ((report.loc[i+1,'Quarter Number']) + 1)) and (report.loc[i+3,'Quarter Number'] == ((report.loc[i+2,'Quarter Number']) + 1)):\n",
    "                    for j in annualizing_list:\n",
    "                        report.loc[i,f'{j} (annualized)'] = report.loc[i,f'{j}'] + report.loc[i+1,f'{j}'] + report.loc[i+2,f'{j}'] + report.loc[i+3,f'{j}']\n",
    "                else:\n",
    "                    for j in annualizing_list:\n",
    "                        report.loc[i,f'{j} (annualized)'] = ''\n",
    "            else:\n",
    "                for j in annualizing_list:\n",
    "                    report.loc[i,f'{j} (annualized)'] = ''\n",
    "        else:\n",
    "            for j in annualizing_list:\n",
    "                report.loc[i,f'{j} (annualized)'] = ''\n",
    "    report = report.drop(columns=annualizing_list)\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat list parameter yg akan di annualizing \n",
    "a_list = ['Total Pendapatan','Laba Kotor','Laba Usaha','Laba Sebelum Pajak','Laba Bersih Tahun Berjalan',\n",
    "          'Arus Kas Dari Aktivitas Operasi','Arus Kas Dari Aktivitas Investasi','Arus Kas Dari Aktivitas Pendanaan']\n",
    "lapkeu_sb = annualizing(report=lapkeu_sb,annualizing_list=a_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Buat agar tiap saham punya jumlah Quarter Year yg sama, supaya begitu join dgn price data tidak ada saham yg skip\n",
    "quarter_year_list = list(lapkeu_sb['Quarter Year'].unique())\n",
    "lapkeu_sb_i_list = []\n",
    "for i in lapkeu_sb['Kode'].unique():\n",
    "    lapkeu_sb_i = lapkeu_sb[lapkeu_sb['Kode']==i]\n",
    "    quarter_year_list_i = list(lapkeu_sb_i['Quarter Year'])\n",
    "    quarter_year_list_not_in_i = [x for x in quarter_year_list if x not in quarter_year_list_i]\n",
    "    df_lapkeu_sb_i = pd.DataFrame(index=list(range(len(quarter_year_list_not_in_i))), columns=lapkeu_sb_i.columns)\n",
    "    df_lapkeu_sb_i['Quarter Year'] = quarter_year_list_not_in_i\n",
    "    df_lapkeu_sb_i['Kode'] = lapkeu_sb_i['Kode'].iloc[0]\n",
    "    df_lapkeu_sb_i['Quarter'] = df_lapkeu_sb_i['Quarter Year'].str[:2]\n",
    "    df_lapkeu_sb_i['Year'] = df_lapkeu_sb_i['Quarter Year'].str[2:]\n",
    "    lapkeu_sb_i = pd.concat([lapkeu_sb_i,df_lapkeu_sb_i])\n",
    "    lapkeu_sb_i = lapkeu_sb_i.sort_values(by=['Year','Quarter'],ascending=False).reset_index(drop=True)\n",
    "    lapkeu_sb_i_list.append(lapkeu_sb_i)\n",
    "lapkeu_sb = pd.concat(lapkeu_sb_i_list)\n",
    "lapkeu_sb = lapkeu_sb.reset_index(drop=True)\n",
    "\n",
    "#Buat Date From dan Date To\n",
    "for i in range(len(lapkeu_sb)):\n",
    "    if lapkeu_sb.loc[i,'Quarter'] == 'Q1':\n",
    "        quarter_year = lapkeu_sb.loc[i,'Year']\n",
    "        lapkeu_sb.loc[i,'Date From'] = f'{quarter_year}-01-01'\n",
    "        lapkeu_sb.loc[i,'Date To'] = f'{quarter_year}-03-31'\n",
    "    elif lapkeu_sb.loc[i,'Quarter'] == 'Q2':\n",
    "        quarter_year = lapkeu_sb.loc[i,'Year']\n",
    "        lapkeu_sb.loc[i,'Date From'] = f'{quarter_year}-04-01'\n",
    "        lapkeu_sb.loc[i,'Date To'] = f'{quarter_year}-06-30'\n",
    "    elif lapkeu_sb.loc[i,'Quarter'] == 'Q3':\n",
    "        quarter_year = lapkeu_sb.loc[i,'Year']\n",
    "        lapkeu_sb.loc[i,'Date From'] = f'{quarter_year}-07-01'\n",
    "        lapkeu_sb.loc[i,'Date To'] = f'{quarter_year}-09-30'\n",
    "    elif lapkeu_sb.loc[i,'Quarter'] == 'Q4':\n",
    "        quarter_year = lapkeu_sb.loc[i,'Year']\n",
    "        lapkeu_sb.loc[i,'Date From'] = f'{quarter_year}-10-01'\n",
    "        lapkeu_sb.loc[i,'Date To'] = f'{quarter_year}-12-31'\n",
    "    else:\n",
    "        lapkeu_sb.loc[i,'Date From'] = '' \n",
    "        lapkeu_sb.loc[i,'Date To'] = ''\n",
    "\n",
    "lapkeu_sb_annualized = lapkeu_sb.copy()\n",
    "lapkeu_sb_annualized.columns = lapkeu_sb_annualized.columns.str.replace(' (annualized)','',regex=False) #semua data selain balance sheet sudah di-anualizedkan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment Quarter Year yang belum lengkap untuk saham2 yg lapkeu-nya belum dirilis sesuai kuartal terakhir (misal MDKA rilis terakhir Q2 2024, sedangkan saham lain sudah Q3 2024)\n",
    "# Tujuannya agar begitu di-join, data terakhir lebih lengkap karena data terakhir nantinya akan di lag-2\n",
    "date_from_list = list(lapkeu_sb_annualized['Date From'].unique())\n",
    "nearest_date_from = lapkeu_sb_annualized['Date From'].max()\n",
    "nearest_date_from_ = datetime.strptime(nearest_date_from, '%Y-%m-%d').date()\n",
    "date_to_list = list(lapkeu_sb_annualized['Date To'].unique())\n",
    "nearest_date_to = lapkeu_sb_annualized['Date To'].max()\n",
    "quarter_dict = {'03':'Q1','06':'Q2','09':'Q3','12':'Q4'}\n",
    "lapkeu_sb_annualized_c_merged_list = []\n",
    "for c in lapkeu_sb_annualized['Kode'].unique():\n",
    "    lapkeu_sb_annualized_c = lapkeu_sb_annualized[lapkeu_sb_annualized['Kode']==c]\n",
    "    last_date_from = lapkeu_sb_annualized_c['Date From'].iloc[0]\n",
    "    last_date_from_ = datetime.strptime(last_date_from, '%Y-%m-%d').date()\n",
    "    if last_date_from_ < nearest_date_from_:\n",
    "        new_date_from_list = list(filter(lambda d: d > last_date_from, date_from_list))\n",
    "        last_date_to = lapkeu_sb_annualized_c['Date To'].iloc[0]\n",
    "        new_date_to_list = list(filter(lambda d: d > last_date_to, date_to_list))\n",
    "        new_lapkeu_df = pd.DataFrame({'Date From':new_date_from_list,'Date To':new_date_to_list})\n",
    "        new_lapkeu_df['Kode'] = c\n",
    "        new_lapkeu_df['Year'] = new_lapkeu_df['Date To'].str[:4]\n",
    "        new_lapkeu_df['Quarter'] = new_lapkeu_df['Date To'].str[5:7]\n",
    "        new_lapkeu_df['Quarter'] = new_lapkeu_df['Quarter'].map(quarter_dict)\n",
    "        new_lapkeu_df['Quarter Year'] = new_lapkeu_df['Quarter'].astype('str') + new_lapkeu_df['Year'].astype('str')\n",
    "        lapkeu_sb_annualized_c_merged = lapkeu_sb_annualized_c.merge(new_lapkeu_df,on=['Kode','Quarter','Year','Quarter Year','Date From','Date To'],how='outer')\n",
    "        lapkeu_sb_annualized_c_merged = lapkeu_sb_annualized_c_merged.sort_values(by='Date From',ascending=False).reset_index(drop=True)\n",
    "        lapkeu_sb_annualized_c_merged_list.append(lapkeu_sb_annualized_c_merged)\n",
    "    else: \n",
    "        lapkeu_sb_annualized_c_merged_list.append(lapkeu_sb_annualized_c)\n",
    "lapkeu_sb_annualized = pd.concat(lapkeu_sb_annualized_c_merged_list)\n",
    "lapkeu_sb_annualized['Quarter Number'] = [quarter_num[x][0] for x in lapkeu_sb_annualized['Quarter Year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ends with _x: {list(lapkeu_sb_annualized.columns[lapkeu_sb_annualized.columns.str.endswith('_x')])}\")\n",
    "print(f\"ends with _y: {list(lapkeu_sb_annualized.columns[lapkeu_sb_annualized.columns.str.endswith('_y')])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pilih LK untuk emiten yang sudah difilter sebelumnya agar ringan dalam menjoin data _x dan _y\n",
    "\n",
    "lapkeu_list = []\n",
    "for i in lapkeu_sb_annualized['Kode'].unique():\n",
    "    lapkeu_i = lapkeu_sb_annualized[lapkeu_sb_annualized['Kode'] == i].copy()\n",
    "    \n",
    "    merged_cols = {}  # Dictionary to store merged columns\n",
    "    \n",
    "    for col in lapkeu_i.columns:\n",
    "        if col.endswith('_x'):\n",
    "            base_col = col[:-2]  # Remove '_x'\n",
    "            y_col = base_col + '_y'\n",
    "            if y_col in lapkeu_i.columns:\n",
    "                merged_cols[base_col] = lapkeu_i[col].fillna(lapkeu_i[y_col])  # Merge _x and _y\n",
    "            else:\n",
    "                merged_cols[col] = lapkeu_i[col]  # Keep original column if _y doesn't exist\n",
    "        elif col.endswith('_y') and col[:-2] not in merged_cols:\n",
    "            merged_cols[col[:-2]] = lapkeu_i[col]  # Keep _y only if _x didn't exist\n",
    "        else:\n",
    "            merged_cols[col] = lapkeu_i[col]  # Keep other columns unchanged\n",
    "\n",
    "    # Convert dictionary to DataFrame and append\n",
    "    lapkeu_list.append(pd.DataFrame(merged_cols))\n",
    "\n",
    "lapkeu_sb_annualized = pd.concat(lapkeu_list)\n",
    "lapkeu_sb_annualized.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"lapkeu_sb: {np.round(lapkeu_sb_annualized.memory_usage(deep=True).sum()/1e9,2)} Gigabytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ends with _x: {list(lapkeu_sb_annualized.columns[lapkeu_sb_annualized.columns.str.endswith('_x')])}\")\n",
    "print(f\"ends with _y: {list(lapkeu_sb_annualized.columns[lapkeu_sb_annualized.columns.str.endswith('_y')])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bersihkan double space supaya pas nyari column nanti ga riweuh sorangan\n",
    "\n",
    "lapkeu_sb_annualized = lapkeu_sb_annualized[[item for item in lapkeu_sb_annualized.columns if not re.search(r'\\s{2,}', item)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameter yg ada di list_growth_params,\n",
    "                list_lag_params,\n",
    "                dan list_selected_params harus sudah annualized !!!\n",
    "Harus ada di a_list !! (Simplified)\n",
    "\"\"\"\n",
    "\n",
    "# akun_akun = ['Aset Lancar','Aset Tidak Lancar','Aset','Liabilitas Jangka Pendek','Liabilitas Jangka Panjang','Liabilitas',\n",
    "#         'Ekuitas','Liabilitas Dan Ekuitas','Kas Dan Setara Kas','Piutang Premi','Piutang Reasuransi','Penempatan Pada Bank Indonesia Dan Bank Lain',\n",
    "#         'Pinjaman Yang Diberikan','Aset Tetap','Piutang Sewa Pembiayaan','Piutang Pembiayaan Konsumen','Piutang Nasabah','Piutang Lain-Lain',\n",
    "#         'Portofolio Efek','Aset Tak Berwujud','Investasi Jangka Pendek','Aset Reasuransi','Piutang Usaha','Piutang Premi Dan Reasuransi',\n",
    "#         'Investasi Jangka Panjang','Goodwill','Piutang Pembiayaan','Obligasi Pemerintah','Investasi Pada Entitas Asosiasi','Deposito Berjangka',\n",
    "#         'Efek Tersedia Untuk Dijual','Total Pendapatan','Laba Kotor','Laba Usaha','Laba Sebelum Pajak','Laba Bersih Tahun Berjalan',\n",
    "#         'Arus Kas Dari Aktivitas Operasi','Arus Kas Dari Aktivitas Investasi','Arus Kas Dari Aktivitas Pendanaan']\n",
    "\n",
    "akun_akun = a_list\n",
    "\n",
    "list_growth_params = akun_akun \n",
    "list_lag_params = akun_akun\n",
    "\n",
    "list_selected_params = ['Kode','Date','Quarter Year','Saham Beredar'] + [x + ' Growth YoY' for x in akun_akun]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Account finder\n",
    "target = \"Total\"\n",
    "\n",
    "matches = [col for col in lapkeu_sb_annualized.columns if target in col]\n",
    "print(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitung Growth YoY\n",
    "\n",
    "list_lapkeu = []\n",
    "for i in set(list(lapkeu_sb_annualized['Kode'])):\n",
    "    lapkeu_i = lapkeu_sb_annualized[lapkeu_sb_annualized['Kode']==i]\n",
    "    lapkeu_i_lag_2 = lapkeu_i[['Kode']]\n",
    "    for j in list_growth_params:\n",
    "        lapkeu_i_lag_2.loc[:,f'{j} Lag 2'] = lapkeu_i[f'{j}']\n",
    "    lapkeu_i_lag_2.loc[:,'Quarter Number'] = lapkeu_i['Quarter Number'] - 2 #Buat lag Quarter Number untuk join\n",
    "    lapkeu_i_lag_2.drop(columns=['Kode'],inplace=True)\n",
    "    lapkeu_i = lapkeu_i.merge(lapkeu_i_lag_2, how='outer', on='Quarter Number')\n",
    "    lapkeu_i_lag_6 = lapkeu_i[['Kode']]\n",
    "    for j in list_growth_params:\n",
    "        lapkeu_i_lag_6.loc[:,f'{j} Lag 6'] = lapkeu_i[f'{j}']\n",
    "    lapkeu_i_lag_6.loc[:,'Quarter Number'] = lapkeu_i.loc[:,'Quarter Number'] - 6 #Buat lag Quarter Number untuk join\n",
    "    lapkeu_i_lag_6.drop(columns=['Kode'],inplace=True)\n",
    "    lapkeu_i = lapkeu_i.merge(lapkeu_i_lag_6, how='outer', on='Quarter Number')  \n",
    "    list_lapkeu.append(lapkeu_i)\n",
    "lapkeu = pd.concat(list_lapkeu).sort_values(by=['Kode','Quarter Number'])\n",
    "lapkeu = lapkeu[~lapkeu['Kode'].isna()]\n",
    "lapkeu = lapkeu.reset_index(drop=True)\n",
    "\n",
    "# Hitung growth\n",
    "for j in list_growth_params:\n",
    "    lapkeu.loc[:,f'{j} Growth YoY'] = (lapkeu[f'{j} Lag 2'].replace('',np.nan).astype('float32')-lapkeu[f'{j} Lag 6'].replace('',np.nan).astype('float32'))/lapkeu[f'{j} Lag 6'].replace('',np.nan).astype('float32')\n",
    "    lapkeu = lapkeu.drop(columns=[f'{j} Lag 6',f'{j} Lag 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Buat Lag 2\n",
    "list_lapkeu = []\n",
    "for i in set(list(lapkeu['Kode'])):\n",
    "    lapkeu_i = lapkeu[lapkeu['Kode']==i]\n",
    "    #Buat lag 2\n",
    "    lapkeu_i_lag_2 = lapkeu_i[['Kode']]\n",
    "    for j in list_lag_params:\n",
    "        lapkeu_i_lag_2.loc[:,f'{j} Lag 2'] = lapkeu_i[f'{j}'].values\n",
    "    lapkeu_i_lag_2.loc[:,'Quarter Number'] = lapkeu_i['Quarter Number'] - 2 #Buat lag Quarter Number untuk join\n",
    "    lapkeu_i_lag_2.drop(columns=['Kode'],inplace=True)\n",
    "    lapkeu_i = lapkeu_i.merge(lapkeu_i_lag_2, how='outer', on='Quarter Number')\n",
    "    list_lapkeu.append(lapkeu_i)\n",
    "lapkeu = pd.concat(list_lapkeu).sort_values(by=['Kode','Quarter Number'])\n",
    "lapkeu = lapkeu[~lapkeu['Kode'].isna()]\n",
    "lapkeu = lapkeu.reset_index(drop=True)\n",
    "for j in list_lag_params:\n",
    "    lapkeu = lapkeu.drop(columns=f'{j}')\n",
    "    lapkeu = lapkeu.rename(columns={f'{j} Lag 2':f'{j}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"lapkeu: {np.round(lapkeu.memory_usage(deep=True).sum()/1e9,2)} Gigabytes\\n\"\n",
    "      f\"lapkeu shape: {lapkeu.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_limit = 2021\n",
    "\n",
    "df_yl = df_all[pd.to_datetime(df_all['portdate']).apply(lambda x: x.year) >= year_limit]\n",
    "lapkeu_yl = lapkeu[lapkeu['Year'].astype(int) >= year_limit]\n",
    "\n",
    "print(f\"df_yl: {np.round(df_yl.memory_usage(deep=True).sum()/1e9,2)} Gigabytes\\n\"\n",
    "      f\"lapkeu shape: {df_yl.shape}\")\n",
    "\n",
    "print(f\"lapkeu_yl: {np.round(lapkeu_yl.memory_usage(deep=True).sum()/1e9,2)} Gigabytes\\n\"\n",
    "      f\"lapkeu shape: {lapkeu_yl.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRASH MULU DAN ABIS RAM\n",
    "# Buat kolom date\n",
    "list_lapkeu = []\n",
    "for i in set(list(lapkeu_yl['Quarter Year'])):\n",
    "    lapkeu_i = lapkeu_yl[lapkeu_yl['Quarter Year']==i]\n",
    "    date_from = lapkeu_i['Date From'].unique()[0]\n",
    "    date_to = lapkeu_i['Date To'].unique()[0]\n",
    "    list_lapkeu_k = []\n",
    "    for k in pd.bdate_range(start=date_from,end=date_to):\n",
    "        lapkeu_k = lapkeu_i.copy()\n",
    "        lapkeu_k.loc[:,'Date'] = k\n",
    "        list_lapkeu_k.append(lapkeu_k)\n",
    "    lapkeu_k = pd.concat(list_lapkeu_k)\n",
    "    list_lapkeu.append(lapkeu_k)\n",
    "lapkeu_all_date = pd.concat(list_lapkeu).reset_index(drop=True)\n",
    "lapkeu_all_date = lapkeu_all_date.sort_values(by=['Date','Kode']).reset_index(drop=True)\n",
    "\n",
    "# Merge data dan ubah nama kolom\n",
    "lapkeu_all_date['Kode + Date'] = lapkeu_all_date['Kode'].astype('str') + ' ' + lapkeu_all_date['Date'].astype('str')\n",
    "df_yl['Kode + Date'] = df_yl['portid'].astype('str') + ' ' + df_yl['portdate'].astype('str')\n",
    "database = df_yl.merge(lapkeu_all_date.drop(['Kode','Date'],axis=1), how='left', on='Kode + Date')\n",
    "database = database.drop('Kode + Date',axis=1)\n",
    "database = database.rename(columns={'portid':'Kode','portdate':'Date','opening':'Open Price','high':'High Price','low':'Low Price','closing':'Close Price','valuess':'Transaction Value'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"database: {np.round(database.memory_usage(deep=True).sum()/1e9,2)} Gigabytes\\n\"\n",
    "      f\"lapkeu shape: {database.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TrEquity df_all Q_year\n",
    "# df_all['Quarter Year'] = pd.to_datetime(df_all['portdate']).dt.to_period('Q').astype(str)\n",
    "# df_all['Quarter Year'] = df_all['Quarter Year'].str[-1] + df_all['Quarter Year'].str[:4]  # Rearrange format\n",
    "# df_all['Quarter Year'] = 'Q' + df_all['Quarter Year']  # Add \"Q\" prefix\n",
    "\n",
    "# # Match Code\n",
    "# lapkeu['KodeQuarterYear'] = lapkeu['Kode'] + lapkeu['Quarter Year']\n",
    "# df_all['KodeQuarterYear'] = df_all['portid'] + df_all['Quarter Year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"df_all: {np.round(df_all.memory_usage(deep=True).sum()/1e9,2)} Gigabytes\\n\"\n",
    "      f\"lapkeu shape: {df_all.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lapkeu_all_date = pd.merge(df_yl, lapkeu_yl, on=\"KodeQuarterYear\", how=\"outer\")\n",
    "lapkeu_all_date = lapkeu_all_date[~lapkeu_all_date['portdate'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lapkeu_all_date[lapkeu_all_date['portid'] == 'ASII'].groupby(['Year', 'Quarter']).last().to_excel(\"ASII db.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # CRASH MULU DAN ABIS RAM\n",
    "# # Buat kolom date\n",
    "# list_lapkeu = []\n",
    "# for i in set(list(lapkeu_yl['Quarter Year'])):\n",
    "#     lapkeu_i = lapkeu_yl[lapkeu_yl['Quarter Year']==i]\n",
    "#     date_from = lapkeu_i['Date From'].unique()[0]\n",
    "#     date_to = lapkeu_i['Date To'].unique()[0]\n",
    "#     list_lapkeu_k = []\n",
    "#     for k in pd.bdate_range(start=date_from,end=date_to):\n",
    "#         lapkeu_k = lapkeu_i.copy()\n",
    "#         lapkeu_k.loc[:,'Date'] = k\n",
    "#         list_lapkeu_k.append(lapkeu_k)\n",
    "#     lapkeu_k = pd.concat(list_lapkeu_k)\n",
    "#     list_lapkeu.append(lapkeu_k)\n",
    "# lapkeu_all_date = pd.concat(list_lapkeu).reset_index(drop=True)\n",
    "# lapkeu_all_date = lapkeu_all_date.sort_values(by=['Date','Kode']).reset_index(drop=True)\n",
    "\n",
    "# # Merge data dan ubah nama kolom\n",
    "# lapkeu_all_date['Kode + Date'] = lapkeu_all_date['Kode'].astype('str') + ' ' + lapkeu_all_date['Date'].astype('str')\n",
    "# df_yl['Kode + Date'] = df_yl['portid'].astype('str') + ' ' + df_yl['portdate'].astype('str')\n",
    "# database = df_yl.merge(lapkeu_all_date.drop(['Kode','Date'],axis=1), how='left', on='Kode + Date')\n",
    "# database = database.drop('Kode + Date',axis=1)\n",
    "# database = database.rename(columns={'portid':'Kode','portdate':'Date','opening':'Open Price','high':'High Price','low':'Low Price','closing':'Close Price','valuess':'Transaction Value'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lapkeu_all_date = pd.merge(df_all, lapkeu, on=\"KodeQuarterYear\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"lapkeu_all_date: {np.round(lapkeu_all_date.memory_usage(deep=True).sum()/1e9,2)} Gigabytes\\n\"\n",
    "#       f\"lapkeu shape: {lapkeu_all_date.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YoY_list = list(lapkeu_all_date.columns[lapkeu_all_date.columns.str.contains(\"YoY\")])\n",
    "\n",
    "# important_columns = ['portid', 'portdate', 'Quarter', 'Year',\n",
    "#                      'opening', 'high', 'low', 'closing', 'valuess', 'volume',\n",
    "#                     'Saham Beredar', 'Total Aset', 'Total Liabilitas', 'Total Ekuitas'] + a_list + YoY_list\n",
    "\n",
    "# database = lapkeu_all_date[important_columns]\n",
    "# database = database.rename(columns={'portid':'Kode','portdate':'Date','opening':'Open Price','high':'High Price','low':'Low Price','closing':'Close Price','valuess':'Transaction Value'})\n",
    "# database = database.sort_values(['Kode','Date']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"database: {np.round(database.memory_usage(deep=True).sum()/1e9,2)} Gigabytes\\n\"\n",
    "#       f\"lapkeu shape: {database.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CRASH MULU DAN ABIS RAM\n",
    "# # Buat kolom date\n",
    "# list_lapkeu = []\n",
    "# for i in set(list(lapkeu['Quarter Year'])):\n",
    "#     lapkeu_i = lapkeu[lapkeu['Quarter Year']==i]\n",
    "#     date_from = lapkeu_i['Date From'].unique()[0]\n",
    "#     date_to = lapkeu_i['Date To'].unique()[0]\n",
    "#     list_lapkeu_k = []\n",
    "#     for k in pd.bdate_range(start=date_from,end=date_to):\n",
    "#         lapkeu_k = lapkeu_i.copy()\n",
    "#         lapkeu_k.loc[:,'Date'] = k\n",
    "#         list_lapkeu_k.append(lapkeu_k)\n",
    "#     lapkeu_k = pd.concat(list_lapkeu_k)\n",
    "#     list_lapkeu.append(lapkeu_k)\n",
    "# lapkeu_all_date = pd.concat(list_lapkeu).reset_index(drop=True)\n",
    "# lapkeu_all_date = lapkeu_all_date.sort_values(by=['Date','Kode']).reset_index(drop=True)\n",
    "\n",
    "# # Merge data dan ubah nama kolom\n",
    "# lapkeu_all_date['Kode + Date'] = lapkeu_all_date['Kode'].astype('str') + ' ' + lapkeu_all_date['Date'].astype('str')\n",
    "# df_all['Kode + Date'] = df_all['portid'].astype('str') + ' ' + df_all['portdate'].astype('str')\n",
    "# database = df_all.merge(lapkeu_all_date.drop(['Kode','Date'],axis=1), how='left', on='Kode + Date')\n",
    "# database = database.drop('Kode + Date',axis=1)\n",
    "# database = database.rename(columns={'portid':'Kode','portdate':'Date','opening':'Open Price','high':'High Price','low':'Low Price','closing':'Close Price','valuess':'Transaction Value'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Only include selected columns\n",
    "# lapkeu_all_date = lapkeu_all_date.loc[:,list_selected_params] \n",
    "# df_all = df_all.loc[:,list_selected_market_params] \n",
    "\n",
    "#Merge data dan ubah nama kolom\n",
    "lapkeu_all_date['Kode + Date'] = lapkeu_all_date['Kode'].astype('str') + ' ' + lapkeu_all_date['Date'].astype('str')\n",
    "df_all['Kode + Date'] = df_all['portid'].astype('str') + ' ' + df_all['portdate'].astype('str')\n",
    "database = df_all.merge(lapkeu_all_date.drop(['Kode','Date'],axis=1), how='left', on='Kode + Date')\n",
    "database = database.drop('Kode + Date',axis=1)\n",
    "database = database.rename(columns={'portid':'Kode','portdate':'Date','opening':'Open Price','high':'High Price','low':'Low Price','closing':'Close Price','valuess':'Transaction Value'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ganti saham beredar menjadi saham beredar yang terakhir, untuk perkalian market cap\n",
    "stocks = pd.DataFrame(database['Kode'].sort_values().unique(),columns=['Kode'])\n",
    "out_shares_dic = {}\n",
    "for i in stocks['Kode']:\n",
    "    d = database[database['Kode']==i]['Saham Beredar'].reset_index(drop=True)\n",
    "    try:\n",
    "        out_shares_dic[i] = d.iloc[d.last_valid_index()]\n",
    "    except TypeError:\n",
    "        out_shares_dic[i] = np.nan\n",
    "out_shares = pd.DataFrame(out_shares_dic.items(),columns=['Kode','Last Outstanding Shares'])\n",
    "database = database.merge(out_shares,how='inner', on='Kode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Buat market cap, log market cap, dan estimated transaction value\n",
    "database['Market Cap'] = database['Close Price']*database['Last Outstanding Shares']\n",
    "database['Log Market Cap'] = np.log10(database['Market Cap'])\n",
    "database['Estimated Transaction Value'] = ((database['High Price']+database['Low Price'])/2)*database['volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Buat kolom momentum untuk berbagai periode. Asumsi 1 bulan = 20 hari\n",
    "database_mom = []\n",
    "for i in set(list(database['Kode'])):\n",
    "    database_i = database[database['Kode']==i]\n",
    "    database_i['Momentum 1 Day'] = database_i['Close Price'].pct_change(periods=1)\n",
    "    database_i['Momentum 3 Day'] = database_i['Close Price'].pct_change(periods=3)\n",
    "    database_i['Momentum 5 Day'] = database_i['Close Price'].pct_change(periods=5)\n",
    "    database_i['Momentum 10 Day'] = database_i['Close Price'].pct_change(periods=10)\n",
    "    database_i['Momentum 1 Month'] = database_i['Close Price'].pct_change(periods=20)\n",
    "    database_i['Momentum 2 Month'] = database_i['Close Price'].pct_change(periods=40)\n",
    "    database_i['Momentum 3 Month'] = database_i['Close Price'].pct_change(periods=60)\n",
    "    database_i['Momentum 6 Month'] = database_i['Close Price'].pct_change(periods=120)\n",
    "    database_i['Momentum 1 Year'] = database_i['Close Price'].pct_change(periods=240)\n",
    "    database_i['Momentum 12-2'] = database_i['Close Price'].pct_change(periods=220).shift(20)\n",
    "    database_i['Total Return 1 Day'] = database_i['Close Price'].pct_change(periods=1).shift(-1)\n",
    "    database_i['Total Return 3 Day'] = database_i['Close Price'].pct_change(periods=3).shift(-3)\n",
    "    database_i['Total Return 5 Day'] = database_i['Close Price'].pct_change(periods=5).shift(-5)\n",
    "    database_i['Total Return 10 Day'] = database_i['Close Price'].pct_change(periods=10).shift(-10)\n",
    "    database_i['Total Return 1 Month'] = database_i['Close Price'].pct_change(periods=20).shift(-20)\n",
    "    database_i['Total Return 2 Month'] = database_i['Close Price'].pct_change(periods=40).shift(-40)\n",
    "    database_i['Total Return 3 Month'] = database_i['Close Price'].pct_change(periods=60).shift(-60)\n",
    "    database_i['Total Return 6 Month'] = database_i['Close Price'].pct_change(periods=120).shift(-120)\n",
    "    database_i['Change in Estimated Transaction Value 1 Month'] = database_i['Estimated Transaction Value'].pct_change(periods=20)\n",
    "    database_mom.append(database_i)\n",
    "database = pd.concat(database_mom).sort_values(by=['Kode','Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_average(df,param,period):\n",
    "    rolling_dic = {'1 Month':20,'3 Month':60,'6 Month':120,'1 Year':240}\n",
    "    init_list = ['Kode','Date']\n",
    "    init_list.append(param)\n",
    "    df['Kode + Date'] = df['Kode'] + df['Date'].astype('str')\n",
    "    df_ = df[init_list]\n",
    "    list_df = []\n",
    "    for i in set(list(df_['Kode'])):\n",
    "        df_i = df_[df_['Kode']==i]\n",
    "        df_i[f'Average {param} {period}'] = df_i[f'{param}'].rolling(rolling_dic[period]).mean()\n",
    "        list_df.append(df_i)\n",
    "    df__ = pd.concat(list_df).sort_values(by=['Kode','Date'])\n",
    "    df__['Kode + Date'] = df__['Kode'].astype('str') + df__['Date'].astype('str')\n",
    "    df__ = df__.drop_duplicates(subset='Kode + Date')\n",
    "    df = df.merge(df__[[f'Average {param} {period}','Kode + Date']],on='Kode + Date',how='left')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = rolling_average(df=database,param='Market Cap',period='1 Month')\n",
    "database = rolling_average(df=database,param='Market Cap',period='6 Month')\n",
    "database = rolling_average(df=database,param='Market Cap',period='1 Year')\n",
    "database = rolling_average(df=database,param='Transaction Value',period='1 Month')\n",
    "database = rolling_average(df=database,param='Transaction Value',period='6 Month')\n",
    "database = rolling_average(df=database,param='Transaction Value',period='1 Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Buat PBV\n",
    "database['PBV'] = database['Market Cap']/database['Total Ekuitas']\n",
    "database['Average PBV 1 Month'] = database['Average Market Cap 1 Month']/database['Total Ekuitas']\n",
    "database['Average PBV 6 Month'] = database['Average Market Cap 6 Month']/database['Total Ekuitas']\n",
    "\n",
    "#Buat Price/Sales\n",
    "database['Price/Sales'] = database['Market Cap']/database['Total Pendapatan'].replace('',np.nan).astype(float)\n",
    "database['Average Price/Sales 1 Month'] = database['Average Market Cap 1 Month']/database['Total Pendapatan'].replace('',np.nan).astype(float)\n",
    "database['Average Price/Sales 6 Month'] = database['Average Market Cap 6 Month']/database['Total Pendapatan'].replace('',np.nan).astype(float)\n",
    "\n",
    "#Buat PE Ratio\n",
    "database['PE Ratio'] = database['Market Cap']/database['Laba Bersih Tahun Berjalan'].replace('',np.nan).astype(float)\n",
    "database['Average PE Ratio 1 Month'] = database['Average Market Cap 1 Month']/database['Laba Bersih Tahun Berjalan'].replace('',np.nan).astype(float)\n",
    "\n",
    "#Buat Price/Operating Cash Flow\n",
    "database['Price/Operating Cash Flow'] = database['Market Cap']/database['Arus Kas Dari Aktivitas Operasi'].replace('',np.nan).astype(float)\n",
    "database['Average Price/Operating Cash Flow 1 Month'] = database['Average Market Cap 1 Month']/database['Arus Kas Dari Aktivitas Operasi'].replace('',np.nan).astype(float)\n",
    "\n",
    "#Buat ROE\n",
    "database['ROE'] = database['Laba Bersih Tahun Berjalan'].replace('',np.nan).astype(float)/database['Total Ekuitas']\n",
    "\n",
    "#Buat Gross, Operating, dan Net Profit Margin\n",
    "database['Net Profit Margin'] = database['Laba Bersih Tahun Berjalan'].replace('',np.nan).astype(float)/database['Total Pendapatan'].replace('',np.nan).astype(float)\n",
    "database['Gross Profit Margin'] = database['Laba Kotor'].replace('',np.nan).astype(float)/database['Total Pendapatan'].replace('',np.nan).astype(float)\n",
    "database['Operating Profit Margin'] = database['Laba Usaha'].replace('',np.nan).astype(float)/database['Total Pendapatan'].replace('',np.nan).astype(float)\n",
    "\n",
    "#Buat Debt Ratio\n",
    "database['Debt Ratio'] = database['Total Liabilitas']/database['Total Aset']\n",
    "\n",
    "# # Tambahkan IHSG\n",
    "# ihsg = pd.read_excel('IHSG.xls',skiprows=[0])\n",
    "# ihsg = ihsg[['Tanggal','Nilai']]\n",
    "# ihsg = ihsg.rename(columns={'Tanggal':'Date','Nilai':'IHSG'})\n",
    "# ihsg['IHSG 1 Day Return'] = ihsg['IHSG'].pct_change()\n",
    "# ihsg = ihsg.dropna()\n",
    "# ihsg['Date'] = ihsg['Date'].astype('str')\n",
    "# ihsg = ihsg.reset_index(drop=True)\n",
    "# database = database.merge(ihsg, how='left', on='Date')\n",
    "\n",
    "# Tambahkan Risk-Free Rate\n",
    "# rf = pd.read_excel('Suku Bunga Deposito 1 Bulan (LPS).xls',skiprows=[0])\n",
    "# rf['Rf (annualized)'] = rf['Nilai']*0.01\n",
    "# rf['Rf'] = ((rf['Nilai']*0.01)+1)**(1/12)-1\n",
    "# rf = rf.rename(columns={'Tanggal':'Date'})\n",
    "# rf = rf.drop(columns=['Indeks','Nilai'])\n",
    "# rf['Month'] = [x.month for x in rf['Date']]\n",
    "# rf['Year'] = [x.year for x in rf['Date']]\n",
    "# rf['Month + Year'] = rf['Month'].astype('str') + ' ' + rf['Year'].astype('str')\n",
    "# rf = rf.drop_duplicates(subset='Month + Year')\n",
    "# rf = rf.drop(columns='Month + Year')\n",
    "# rf = rf.reset_index(drop=True)\n",
    "# for i in range(len(rf)):\n",
    "#     if rf.loc[i,'Month'] == 1:\n",
    "#         year = rf.loc[i,'Year']\n",
    "#         rf.loc[i,'Date From'] = rf.loc[i,'Date']\n",
    "#         rf.loc[i,'Date To'] = f'{year}-01-31'\n",
    "#     elif rf.loc[i,'Month'] == 2:\n",
    "#         year = rf.loc[i,'Year']\n",
    "#         rf.loc[i,'Date From'] = rf.loc[i,'Date']\n",
    "#         rf.loc[i,'Date To'] = f'{year}-02-28'\n",
    "#     elif rf.loc[i,'Month'] == 3:\n",
    "#         year = lapkeu_sb.loc[i,'Year']\n",
    "#         rf.loc[i,'Date From'] = rf.loc[i,'Date']\n",
    "#         rf.loc[i,'Date To'] = f'{year}-03-31'\n",
    "#     elif rf.loc[i,'Month'] == 4:\n",
    "#         year = rf.loc[i,'Year']\n",
    "#         rf.loc[i,'Date From'] = rf.loc[i,'Date']\n",
    "#         rf.loc[i,'Date To'] = f'{year}-04-30'\n",
    "#     elif rf.loc[i,'Month'] == 5:\n",
    "#         year = rf.loc[i,'Year']\n",
    "#         rf.loc[i,'Date From'] = rf.loc[i,'Date']\n",
    "#         rf.loc[i,'Date To'] = f'{year}-05-31'\n",
    "#     elif rf.loc[i,'Month'] == 6:\n",
    "#         year = rf.loc[i,'Year']\n",
    "#         rf.loc[i,'Date From'] = rf.loc[i,'Date']\n",
    "#         rf.loc[i,'Date To'] = f'{year}-06-30'\n",
    "#     elif rf.loc[i,'Month'] == 7:\n",
    "#         year = rf.loc[i,'Year']\n",
    "#         rf.loc[i,'Date From'] = rf.loc[i,'Date']\n",
    "#         rf.loc[i,'Date To'] = f'{year}-07-31'\n",
    "#     elif rf.loc[i,'Month'] == 8:\n",
    "#         year = rf.loc[i,'Year']\n",
    "#         rf.loc[i,'Date From'] = rf.loc[i,'Date']\n",
    "#         rf.loc[i,'Date To'] = f'{year}-08-31'\n",
    "#     elif rf.loc[i,'Month'] == 9:\n",
    "#         year = rf.loc[i,'Year']\n",
    "#         rf.loc[i,'Date From'] = rf.loc[i,'Date']\n",
    "#         rf.loc[i,'Date To'] = f'{year}-09-30'\n",
    "#     elif rf.loc[i,'Month'] == 10:\n",
    "#         year = rf.loc[i,'Year']\n",
    "#         rf.loc[i,'Date From'] = rf.loc[i,'Date']\n",
    "#         rf.loc[i,'Date To'] = f'{year}-10-31'\n",
    "#     elif rf.loc[i,'Month'] == 11:\n",
    "#         year = rf.loc[i,'Year']\n",
    "#         rf.loc[i,'Date From'] = rf.loc[i,'Date']\n",
    "#         rf.loc[i,'Date To'] = f'{year}-11-30'\n",
    "#     elif rf.loc[i,'Month'] == 12:\n",
    "#         year = rf.loc[i,'Year']\n",
    "#         rf.loc[i,'Date From'] = rf.loc[i,'Date']\n",
    "#         rf.loc[i,'Date To'] = f'{year}-12-31'\n",
    "#     else:\n",
    "#         rf.loc[i,'Date From'] = '' \n",
    "#         rf.loc[i,'Date To'] = ''\n",
    "# list_rf= []\n",
    "# for i in set(list(rf['Date'])):\n",
    "#     rf_i = rf[rf['Date']==i]\n",
    "#     date_from = rf_i['Date From'].unique()[0]\n",
    "#     date_to = rf_i['Date To'].unique()[0]\n",
    "#     list_rf_k = []\n",
    "#     for k in pd.bdate_range(start=date_from,end=date_to):\n",
    "#         rf_k = rf_i.copy()\n",
    "#         rf_k.loc[:,'Date'] = k\n",
    "#         list_rf_k.append(rf_k)\n",
    "#     try:\n",
    "#         rf_k = pd.concat(list_rf_k)\n",
    "#         list_rf.append(rf_k)\n",
    "#     except ValueError:\n",
    "#         pass\n",
    "# rf_all_date = pd.concat(list_rf)\n",
    "# rf_all_date = rf_all_date.sort_values(by=['Date','Rf'])\n",
    "# rf_all_date = rf_all_date.drop_duplicates(subset='Date', keep='first').reset_index(drop=True)\n",
    "# rf_all_date = rf_all_date[['Date','Rf','Rf (annualized)']]\n",
    "# rf_all_date['Date'] = rf_all_date['Date'].astype('str')\n",
    "# database = database.merge(rf_all_date, how='left', on='Date')\n",
    "# database = database.dropna(subset=[\"Kode\"])\n",
    "# database = database.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JIKA MAU HITUNG PARAMETER QUALITY IDXVESTA28, BUKA Create_Database_Infovesta28.ipynb !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tambah Daily Frekuensi (ambil dari Stock Summary BEI, cek 'frekuensi.ipynb')\n",
    "freq_1 = pd.read_csv('Data Frekuensi 1.csv')\n",
    "freq_2 = pd.read_csv('Data Frekuensi 2.csv')\n",
    "freq = pd.concat([freq_1,freq_2])\n",
    "freq = freq.dropna()\n",
    "freq['Kode + Date'] = freq['Kode'] + freq['Date'].astype('str')\n",
    "database['Kode + Date'] = database['Kode'] + database['Date'].astype('str')\n",
    "database = database.merge(freq[['Frequency','Kode + Date']],on='Kode + Date',how='left')\n",
    "database = database.drop(columns='Kode + Date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database[database['Kode']=='BMRI'].groupby(['Quarter', 'Year']).sum(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(database)\n",
    "database = database.sort_values(by='Date')\n",
    "database = database.reset_index(drop=True)\n",
    "database_1 = database.iloc[:int(size/4)]\n",
    "database_2 = database.iloc[int(size/4):int(size/2)]\n",
    "database_3 = database.iloc[int(size/2):int(size*(3/4))]\n",
    "database_4 = database.iloc[int(size*(3/4)):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_1.to_csv('/Users/irfanhilman/Downloads/Database/Database Terbaru/Database Part 1.csv')\n",
    "database_2.to_csv('/Users/irfanhilman/Downloads/Database/Database Terbaru/Database Part 2.csv')\n",
    "database_3.to_csv('/Users/irfanhilman/Downloads/Database/Database Terbaru/Database Part 3.csv')\n",
    "database_4.to_csv('/Users/irfanhilman/Downloads/Database/Database Terbaru/Database Part 4.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_1 = pd.read_csv('/Users/irfanhilman/Downloads/Database/Database Terbaru/Database Part 1.csv')\n",
    "database_2 = pd.read_csv('/Users/irfanhilman/Downloads/Database/Database Terbaru/Database Part 2.csv')\n",
    "database_3 = pd.read_csv('/Users/irfanhilman/Downloads/Database/Database Terbaru/Database Part 3.csv')\n",
    "database_4 = pd.read_csv('/Users/irfanhilman/Downloads/Database/Database Terbaru/Database Part 4.csv')\n",
    "database = pd.concat([database_1,database_2,database_3,database_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database[database['Kode']=='BREN']['Average Transaction Value 1 Year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database[database['Kode']=='GEMS'].to_excel('GEMS.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
