{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_duplicates(data,param_combined,param1,param2):\n",
    "    data[param_combined] = data[param1].astype(str) + ' ' + data[param2].astype(str)\n",
    "    data = data.drop_duplicates(subset=param_combined)\n",
    "    data = data.drop(columns=param_combined)\n",
    "    return data\n",
    "def merge_left(left_data,right_data,param_combined,param1,param2,left_columns,right_columns):\n",
    "    left_data[param_combined] = left_data[param1].astype(str) + ' ' + left_data[param2].astype(str)\n",
    "    right_data[param_combined] = right_data[param1].astype(str) + ' ' + right_data[param2].astype(str)\n",
    "    left_columns.append(param_combined)\n",
    "    left_columns = list(set(left_columns))\n",
    "    right_columns.append(param_combined)\n",
    "    right_columns = list(set(right_columns))\n",
    "    output = left_data[left_columns].merge(right_data[right_columns],on=param_combined,how='left')\n",
    "    left_data = left_data.drop(columns=param_combined)\n",
    "    right_data = right_data.drop(columns=param_combined)\n",
    "    output = output.drop(columns=param_combined)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING\n",
    "(SKIP JIKA SUDAH RUN DATABASE bond_vt !!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rj/b4yv8pv54v31_6jpqn2byxgc0000gn/T/ipykernel_6431/2532939075.py:2: DtypeWarning: Columns (13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  bond_price_1 = pd.read_csv('Bond_TrBei_to_20240816.csv')\n"
     ]
    }
   ],
   "source": [
    "bond_identity = pd.read_csv('bond_msbond.csv')\n",
    "bond_price_1 = pd.read_csv('Bond_TrBei_to_20240816.csv')\n",
    "bond_price_2 = pd.read_csv('Bond_TrBei_from_20240819_to_20250226.csv')\n",
    "bond_price = pd.concat([bond_price_1,bond_price_2])\n",
    "bond = bond_price.merge(bond_identity,on='portId',how='left')\n",
    "bond['issuedyear'] = bond['issueddate'].str[:4]\n",
    "bond['issuedyear'] = bond['issuedyear'].astype('int')\n",
    "bond = bond.drop(columns='txtNo')\n",
    "bond = delete_duplicates(data=bond,param_combined='portId + issueddate',param1='portId',param2='issueddate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bond['portReleased'] = bond['portReleased'].replace(r'\\N',np.nan)\n",
    "bond['jtdatedt'] = pd.to_datetime(bond['jtdate'])\n",
    "bond['portReleaseddt'] = pd.to_datetime(bond['portReleased'])\n",
    "bond['tenorday'] = bond['jtdatedt'] - bond['portReleaseddt']\n",
    "bond['tenorday'] = pd.to_numeric(bond['tenorday'].astype('str').str[:-5],errors='coerce')\n",
    "bond['tenoryear'] = bond['tenorday']/365\n",
    "bond = bond.drop(columns=['jtdatedt','portReleaseddt'])\n",
    "bond = delete_duplicates(data=bond,param_combined='portId + issueddate',param1='portId',param2='issueddate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(bond.columns)\n",
    "bond['tradeday'] = 'Y'\n",
    "bond_i_list = []\n",
    "for i in bond['portId'].unique():\n",
    "    bond_i = bond[bond['portId']==i]\n",
    "    start_date = bond_i['portReleased'].unique()[0]\n",
    "    end_date = max(bond['issueddate'].unique())\n",
    "    jt_date = bond_i['jtdate'].unique()[0]\n",
    "    if type(jt_date) is str:\n",
    "        if end_date>jt_date:\n",
    "            end_date = jt_date\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        print(f'JT date for {i} is not found')\n",
    "        continue\n",
    "    if type(start_date) is str:\n",
    "        bond_i_date = pd.DataFrame({'issueddate':pd.date_range(start=start_date,end=end_date,freq='D')})\n",
    "        if len(bond_i_date)>0:\n",
    "            bond_i_date['issueddate'] = bond_i_date['issueddate'].astype('str')\n",
    "            bond_i = bond_i_date.merge(bond_i,on='issueddate',how='left')\n",
    "            bond_i = bond_i.sort_values(by='issueddate')\n",
    "            bond_i.loc[:,cols] = bond_i.loc[:,cols].ffill()\n",
    "            bond_i_list.append(bond_i)\n",
    "        else:\n",
    "            bond_i_list.append(bond_i)\n",
    "    else:\n",
    "        print(f'Released date for {i} is not found')\n",
    "        continue\n",
    "bond = pd.concat(bond_i_list)\n",
    "bond = bond.dropna(subset='portId')\n",
    "bond = delete_duplicates(data=bond,param_combined='portId + issueddate',param1='portId',param2='issueddate')\n",
    "bond = bond.reset_index(drop=True)\n",
    "bond['tradeday'] = bond['tradeday'].fillna('N')\n",
    "bond['weekend'] = np.where((pd.to_datetime(bond['issueddate'], errors='coerce')).dt.dayofweek<5,'N','Y')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tambah kurs BI\n",
    "kurs_BI = pd.read_excel('kurs_BI.xls')\n",
    "kurs_BI.columns = kurs_BI.loc[0]\n",
    "kurs_BI = kurs_BI[1:].reset_index(drop=True)\n",
    "kurs_BI = kurs_BI.rename(columns={'Nilai': 'USDIDR', 'Tanggal': 'issueddate'})\n",
    "kurs_BI['issueddate'] = kurs_BI['issueddate'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "bond = pd.merge(bond, kurs_BI[['issueddate', 'USDIDR']], on='issueddate', how='left')\n",
    "bond_list = []\n",
    "for i in bond['portId'].unique():\n",
    "    bond_i = bond[bond['portId']==i]\n",
    "    bond_i = bond_i.sort_values(by='issueddate')\n",
    "    bond_i['USDIDR'] = bond_i['USDIDR'].ffill()\n",
    "    bond_list.append(bond_i)\n",
    "bond = pd.concat(bond_list)\n",
    "bond = delete_duplicates(data=bond,param_combined='portId + issueddate',param1='portId',param2='issueddate')\n",
    "bond = bond.reset_index(drop=True)\n",
    "bond['outstanding'] = np.where(bond['dollar']=='Y',bond['outstanding']*bond['USDIDR'],bond['outstanding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tambah nominal\n",
    "bond_nominal = pd.read_csv('bond_msnominal.csv').drop('txtno',axis=1).rename(columns={'portid': 'portId','tanggal': 'issueddate'})\n",
    "bond_nominal['portId + issueddate'] = bond_nominal['portId'].astype(str) + ' ' + bond_nominal['issueddate'].astype(str)\n",
    "bond['portId + issueddate'] = bond['portId'].astype(str) + ' ' + bond['issueddate'].astype(str)\n",
    "bond = bond.merge(bond_nominal[['portId + issueddate','nominal']],how='left',on='portId + issueddate')\n",
    "bond = bond.drop(columns='portId + issueddate')\n",
    "bond_list = []\n",
    "for i in bond['portId'].unique():\n",
    "    bond_i = bond[bond['portId']==i]\n",
    "    bond_i = bond_i.sort_values(by='issueddate')\n",
    "    outstanding_i = bond_i['outstanding'].dropna().unique()[0]\n",
    "    bond_i['nominal'] = bond_i['nominal'].ffill()\n",
    "    bond_i['nominal'] = bond_i['nominal'].fillna(outstanding_i)\n",
    "    bond_list.append(bond_i)\n",
    "bond = pd.concat(bond_list)\n",
    "bond = delete_duplicates(data=bond,param_combined='portId + issueddate',param1='portId',param2='issueddate')\n",
    "bond = bond.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tambah accrued interest\n",
    "bond_list = []\n",
    "for i in bond['portId'].unique():\n",
    "    bond_i = bond[bond['portId']==i]\n",
    "    coupon_i = bond_i['coupon'].unique()[0]*0.01\n",
    "    outstanding_i = bond_i['outstanding'].unique()[0]\n",
    "    bond_i['interest'] = (coupon_i*outstanding_i)/365\n",
    "    bond_i = bond_i.sort_values(by='issueddate')\n",
    "    bond_i['accruedinterest'] = bond_i['interest'].cumsum()\n",
    "    bond_list.append(bond_i)\n",
    "bond = pd.concat(bond_list)\n",
    "bond = delete_duplicates(data=bond,param_combined='portId + issueddate',param1='portId',param2='issueddate')\n",
    "bond = bond.reset_index(drop=True)\n",
    "bond['marketvalue'] = bond['nominal']*bond['lastPrice']*0.01 + bond['accruedinterest']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbn = bond[bond['portType']=='S']\n",
    "corpo = bond[bond['portType']=='B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbn_bday = sbn[sbn['weekend']=='N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate average and sum transaction value over 3 months\n",
    "period = 21*3 #days\n",
    "date_list = [i.strftime('%Y-%m-%d') for i in pd.bdate_range(min(bond['issueddate']),max(bond['issueddate']))]\n",
    "sbn_j_list = []\n",
    "for en,i in enumerate(set(list(sbn_bday['portId']))):\n",
    "    sbn_i = sbn_bday[sbn_bday['portId']==i].drop_duplicates()\n",
    "    sbn_i = sbn_i.sort_values(by='issueddate')\n",
    "    sbn_i['issueddatedt'] = pd.to_datetime(sbn_i['issueddate'])\n",
    "    for j in list(sbn_i['issueddate'].unique()):\n",
    "        sbn_j = sbn_i[sbn_i['issueddate']==j]\n",
    "        try:\n",
    "            pd_pos = date_list.index(j) - period #previous date index position\n",
    "            cd_ = datetime.datetime.strptime(j, '%Y-%m-%d')\n",
    "            pd_ = datetime.datetime.strptime(date_list[pd_pos], '%Y-%m-%d')\n",
    "            sbn_i_inside_dates = sbn_i[(sbn_i['issueddatedt']>=pd_)&(sbn_i['issueddatedt']<=cd_)]\n",
    "            if len(sbn_i_inside_dates)>0:\n",
    "                sbn_j[f'sumvalueTrans_{period}'] = sum(sbn_i_inside_dates['valueTrans'])\n",
    "                sbn_j[f'avgvalueTrans_{period}'] = sum(sbn_i_inside_dates['valueTrans'])/len(sbn_i_inside_dates['valueTrans'])\n",
    "                sbn_j[f'pctdayvalueTrans_{period}'] = len(sbn_i_inside_dates['valueTrans'])/(period+1)\n",
    "                sbn_j_list.append(sbn_j)\n",
    "            else: pass\n",
    "        except ValueError: pass\n",
    "sbn_vt_bday = pd.concat(sbn_j_list)\n",
    "sbn_vt_bday = delete_duplicates(data=sbn_vt_bday,param_combined='portId + issueddate',param1='portId',param2='issueddate')\n",
    "sbn_vt_bday = sbn_vt_bday.reset_index(drop=True)\n",
    "sbn_vt_bday = sbn_vt_bday.drop(columns='issueddatedt')\n",
    "sbn_vt = merge_left(left_data=sbn,right_data=sbn_vt_bday,param_combined='portId + issueddate',param1='portId',param2='issueddate',left_columns=list(sbn.columns),right_columns=['sumvalueTrans_63','avgvalueTrans_63','pctdayvalueTrans_63'])\n",
    "sbn_vt_list = []\n",
    "for i in set(list(sbn_vt['portId'])):\n",
    "    sbn_vt_i = sbn_vt[sbn_vt['portId']==i]\n",
    "    sbn_vt_i = sbn_vt_i.sort_values(by='issueddate')\n",
    "    sbn_vt_i[['sumvalueTrans_63', 'pctdayvalueTrans_63', 'avgvalueTrans_63']] = sbn_vt_i[['sumvalueTrans_63', 'pctdayvalueTrans_63', 'avgvalueTrans_63']].ffill()\n",
    "    sbn_vt_list.append(sbn_vt_i)\n",
    "sbn_vt = pd.concat(sbn_vt_list)\n",
    "sbn_vt = sbn_vt.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sbn_vt.to_csv('bond_database_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INDEXING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbn_vt = pd.read_csv('bond_database_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate portfolio composition\n",
    "data = sbn_vt.copy()\n",
    "start_nab = 10000\n",
    "reb_date = ['2020-01-01','2020-04-01','2020-07-01','2020-10-01'\n",
    "            ,'2021-01-01','2021-04-01','2021-07-01','2021-10-01'\n",
    "            ,'2022-01-01','2022-04-01','2022-07-01','2022-10-01'\n",
    "            ,'2023-01-01','2023-04-01','2023-07-01','2023-10-01'\n",
    "            ,'2024-01-01','2024-04-01','2024-07-01','2024-10-01','2025-02-03'] #Harus Urut dari terkecil ke terbesar! Rebalancing date terakhir tidak dihitung!\n",
    "sort_param = 'sumvalueTrans_63'\n",
    "sort_by = 'High' #'High' untuk top nilai terbesar, 'Low' untuk top nilai terkecil\n",
    "num_assets = 50\n",
    "screening_param = ['pctdayvalueTrans_63']\n",
    "screening_sign = ['Above'] #'Above' or 'Below'\n",
    "screening_val = [0.9]\n",
    "basic_param = ['issueddate','portId','portName','lastPrice','outstanding','coupon','portReleased','jtdate',sort_param]\n",
    "drop_name_list = [] #Drop nama bond tertentu\n",
    "drop_type_list = ['VR','SPN'] #Drop tipe Bond dari string\n",
    "\n",
    "\n",
    "basic_param = list(set(basic_param))\n",
    "extend_param = basic_param.copy()\n",
    "extend_param.extend(screening_param)\n",
    "extend_param = list(set(extend_param))\n",
    "database_part = sbn_vt[extend_param]\n",
    "database_date_list = list(sbn_vt['issueddate'].sort_values().unique())\n",
    "database_date_list_ = [d for d in database_date_list if d >= reb_date[0] and d <= reb_date[-1]]\n",
    "buy_date_list = reb_date[:-1]\n",
    "sell_date_list = []\n",
    "for i in reb_date[1:]:\n",
    "    sell_date_i = database_date_list_.index(i)-1\n",
    "    sell_date = database_date_list_[sell_date_i]\n",
    "    sell_date_list.append(sell_date)\n",
    "param_date_list = []\n",
    "for i in buy_date_list:\n",
    "    param_date_i = database_date_list.index(i)-1\n",
    "    param_date = database_date_list[param_date_i]\n",
    "    param_date_list.append(param_date)\n",
    "database_i_list = []\n",
    "for en,i in enumerate(param_date_list):\n",
    "    database_i = database_part[database_part['issueddate']==i]\n",
    "    database_i = database_i.dropna(subset=['issueddate', 'portId', sort_param] + [f'{s}' for s in screening_param])\n",
    "    if len(drop_type_list)>0:\n",
    "        for t in drop_type_list:\n",
    "            database_i = database_i[~database_i['portId'].str.contains(t)]\n",
    "    else:\n",
    "        pass\n",
    "    if len(drop_name_list)>0:\n",
    "        for r in drop_name_list:\n",
    "            database_i = database_i[database_i['portId']!=n]\n",
    "    else:\n",
    "        pass\n",
    "    if len(screening_param) == len(screening_sign) == len(screening_val):\n",
    "        for ens,j in enumerate(screening_param):  #Initial Screening\n",
    "            if screening_sign[ens] == 'Above':\n",
    "                database_i = database_i[database_i[j]>screening_val[ens]]\n",
    "            elif screening_sign[ens] == 'Below':\n",
    "                database_i = database_i[database_i[j]<screening_val[ens]]\n",
    "            else:\n",
    "                raise TypeError('screening_sign wrong. Select either Above or Below')\n",
    "    else:\n",
    "        raise Exception('Number of elements in screening parameter list are different')\n",
    "    database_i = database_i[basic_param] \n",
    "    if sort_by=='High':\n",
    "        database_i = database_i.sort_values(by=sort_param,ascending=False)\n",
    "        database_i = database_i.head(num_assets)\n",
    "        database_i['rank'] = database_i[sort_param].rank()\n",
    "    elif sort_by=='Low':\n",
    "        database_i = database_i.sort_values(by=sort_param,ascending=True)\n",
    "        database_i = database_i.head(num_assets)\n",
    "        database_i['rank'] = database_i[sort_param].rank(ascending=False)\n",
    "    else:\n",
    "        raise TypeError('sort_param wrong. Check your typing')\n",
    "    if len(database_i) == num_assets:\n",
    "        pass\n",
    "    else:\n",
    "        raise Exception('Number of stocks in one portfolio is less than specified')\n",
    "    database_i['from'] = buy_date_list[en]\n",
    "    database_i['to'] = sell_date_list[en]\n",
    "    database_i['number'] = en\n",
    "    database_i_list.append(database_i)\n",
    "reb_comp_ = pd.concat(database_i_list)\n",
    "reb_comp_ = reb_comp_.reset_index(drop=True)\n",
    "reb_comp = reb_comp_[['portId','from','to','number']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate nama bond di dalam portfolio dan date-nya\n",
    "num_and_date_list = []\n",
    "for i in reb_comp['number'].unique():\n",
    "    reb_comp_i = reb_comp[reb_comp['number']==i]\n",
    "    start_date = reb_comp_i['from'].unique()[0]\n",
    "    end_date = reb_comp_i['to'].unique()[0]\n",
    "    dates = [d.strftime('%Y-%m-%d') for d in pd.date_range(start_date,end_date)]\n",
    "    num_and_date = pd.DataFrame({'issueddate':dates})\n",
    "    num_and_date['number'] = i\n",
    "    num_and_date_list.append(num_and_date)\n",
    "num_and_dates = pd.concat( num_and_date_list)\n",
    "num_and_dates = num_and_dates.sort_values(by='issueddate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indexing. Generate Excel file\n",
    "data = sbn_vt.copy()\n",
    "data_list = []\n",
    "data_i_list = []\n",
    "with pd.ExcelWriter(f'Bond Index.xlsx') as writer:\n",
    "    for en,i in enumerate(num_and_dates['issueddate']):\n",
    "        data_i = data[data['issueddate']==i].drop_duplicates(subset='portId')\n",
    "        num = num_and_dates[num_and_dates['issueddate']==i]['number'].values[0]\n",
    "        bonds = reb_comp[reb_comp['number']==num][['portId']]\n",
    "        data_i = bonds.merge(data_i,on='portId',how='left')\n",
    "        data_i = data_i.dropna(subset='marketvalue') \n",
    "        total_nominal= data_i['nominal'].sum()\n",
    "        total_interest = data_i['interest'].sum()\n",
    "        total_marketvalue = data_i['marketvalue'].sum()\n",
    "        if en<1:\n",
    "            total_accruedinterest = data_i['accruedinterest'].sum()\n",
    "            total_unit = total_marketvalue/start_nab\n",
    "            nab = total_marketvalue/total_unit\n",
    "        else:\n",
    "            data_i_t_1 = data_list[en-1]\n",
    "            total_accruedinterest = total_accruedinterest+total_interest\n",
    "            nab = (total_marketvalue-(total_nominal-data_i_t_1['Total Nominal'].values[0]))/total_unit\n",
    "            total_unit = total_unit+(total_nominal-data_i_t_1['Total Nominal'].values[0])/nab\n",
    "        df = pd.DataFrame({'Date':i,'Day':[pd.to_datetime(i).day_name()],'Total Nominal':[total_nominal],'Total Interest':[total_interest],'Total Accrued Interest':[total_accruedinterest],\n",
    "                                'Total Market Value':[total_marketvalue],'Total Unit':[total_unit],'NAB':[nab]},index=[en])\n",
    "        data_list.append(df)\n",
    "        data_i_list.append(data_i)\n",
    "    bond_index = pd.concat(data_list)\n",
    "    detail = pd.concat(data_i_list)\n",
    "    bond_index.to_excel(writer, sheet_name='NAB')\n",
    "    reb_comp.to_excel(writer, sheet_name='Composition')\n",
    "    detail.to_excel(writer, sheet_name='Detail')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
